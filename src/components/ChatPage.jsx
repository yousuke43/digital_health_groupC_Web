import React, { useState, useRef, useEffect } from 'react';
import VrmViewer from './VrmViewer'; 
import '../styles/ChatPage.css'; // â˜… CSSã®ç¢ºèªãŒå¿…è¦ (å¾Œè¿°)
const IP = import.meta.env.VITE_SERVER_IP;
const SERVER_URL = `ws://${IP}/ws/transcribe`;

// (Web Speech API ã®æº–å‚™ ... å¤‰æ›´ãªã—)
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
let recognition;
if (SpeechRecognition) {
    recognition = new SpeechRecognition();
    recognition.lang = 'ja-JP';
    recognition.interimResults = false;
    recognition.continuous = true; 
} else {
    console.warn("Web Speech API ã¯ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚");
}


function ChatPage() {
  const [status, setStatus] = useState({ key: 'disconnected', text: 'æœªæ¥ç¶š' });
  const [logs, setLogs] = useState([]);
  const [textInput, setTextInput] = useState('');
  const [isRecording, setIsRecording] = useState(false);
  const [audioToPlay, setAudioToPlay] = useState(null);
  const [micError, setMicError] = useState(null);
  
  // â˜…â˜…â˜… 1. ã€Œè€ƒãˆä¸­ã€çŠ¶æ…‹ã‚’è¿½åŠ  â˜…â˜…â˜…
  const [isThinking, setIsThinking] = useState(false);

  const websocket = useRef(null);
  const logContainerRef = useRef(null);
  const vrmViewerRef = useRef(null);
  
  // ãƒ­ã‚°ãŒè¿½åŠ ã•ã‚ŒãŸã‚‰ä¸€ç•ªä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
  useEffect(() => {
    if (logContainerRef.current) {
      logContainerRef.current.scrollTop = logContainerRef.current.scrollHeight;
    }
  }, [logs, isThinking]); // â˜… isThinking ãŒå¤‰ã‚ã£ãŸæ™‚ã‚‚ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«

  // ãƒ­ã‚°ã‚’é…åˆ—ã®ã€Œæœ€å¾Œã€ã«è¿½åŠ 
  const addLog = (text, type) => {
    setLogs(prevLogs => [
      ...prevLogs,
      { id: Date.now(), text, type }
    ]);
  };

  // --- WebSocketæ¥ç¶š ---
  const connect = () => {
    if (vrmViewerRef.current) vrmViewerRef.current.startAudioContext();
    if (websocket.current && websocket.current.readyState !== WebSocket.CLOSED) return;
    setStatus({ key: 'connecting', text: 'æ¥ç¶šä¸­...' });
    websocket.current = new WebSocket(SERVER_URL);
    websocket.current.binaryType = 'arraybuffer';

    websocket.current.onopen = () => {
      setStatus({ key: 'connected', text: 'æ¥ç¶šæ¸ˆã¿' });
      addLog("ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã—ã¾ã—ãŸã€‚", "info");
    };
    websocket.current.onclose = () => {
      setStatus({ key: 'disconnected', text: 'æœªæ¥ç¶š' });
      stopMicrophone();
      addLog("ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰åˆ‡æ–­ã•ã‚Œã¾ã—ãŸã€‚", "info");
      websocket.current = null;
      setIsThinking(false); // â˜… åˆ‡æ–­æ™‚ã‚‚ã€Œè€ƒãˆä¸­ã€ã‚’è§£é™¤
    };
    websocket.current.onerror = (event) => {
      console.error("WebSocketã‚¨ãƒ©ãƒ¼:", event);
      setStatus({ key: 'disconnected', text: 'ã‚¨ãƒ©ãƒ¼' });
      setIsThinking(false); // â˜… ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã€Œè€ƒãˆä¸­ã€ã‚’è§£é™¤
    };

    // â˜…â˜…â˜… 2. onmessage ãƒãƒ³ãƒ‰ãƒ©ã‚’ä¿®æ­£ â˜…â˜…â˜…
    websocket.current.onmessage = (event) => {
      if (typeof event.data === 'string') {
        try {
          const data = JSON.parse(event.data);
          
          if (data.type === "user_transcription") {
            addLog(data.text, 'user');
          
          } else if (data.type === "ai_processing") {
            // â˜… ã€Œè€ƒãˆä¸­ã€ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ (ãƒ­ã‚°ã«ã¯è¿½åŠ ã—ãªã„)
            setIsThinking(true); 
            
          } else if (data.type === "ai_response") {
            // â˜… AIã®è¿”äº‹ãŒæ¥ãŸã‚‰ã€Œè€ƒãˆä¸­ã€ã‚’è§£é™¤ã—ã€ãƒ­ã‚°ã«è¿½åŠ 
            setIsThinking(false); 
            addLog(data.text, 'ai');
          }
          
        } catch (e) { 
          setIsThinking(false); // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚è§£é™¤
          addLog(event.data, 'ai'); 
        }
      } else if (event.data instanceof ArrayBuffer) {
        // â˜… éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒæ¥ã¦ã‚‚ã€Œè€ƒãˆä¸­ã€ã‚’è§£é™¤
        setIsThinking(false); 
        setAudioToPlay(event.data);
      }
    };
  }; // connect é–¢æ•°ã®çµ‚ã‚ã‚Š

  const disconnect = () => {
    if (websocket.current && websocket.current.readyState === WebSocket.OPEN) {
      websocket.current.close();
    }
  };

  // --- ãƒã‚¤ã‚¯å‡¦ç† (Web Speech API) ---
  useEffect(() => {
    if (!recognition) {
      setMicError('ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚');
      addLog('éŸ³å£°èªè­˜éå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ã§ã™', 'info');
      return;
    }
    // ( ... onstart, onend, onspeechstart, onspeechend ã¯å¤‰æ›´ãªã— ...)
    recognition.onstart = () => { console.log('SpeechRecognition: onstart'); setIsRecording(true); addLog("ãƒã‚¤ã‚¯éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚", "info"); };
    recognition.onend = () => { console.log('SpeechRecognition: onend'); setIsRecording(false); addLog("ãƒã‚¤ã‚¯ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚", "info"); };
    recognition.onspeechstart = () => { console.log('SpeechRecognition: onspeechstart'); addLog("éŸ³å£°ã®æ¤œå‡ºã‚’é–‹å§‹ã—ã¾ã—ãŸ...", "info"); };
    recognition.onspeechend = () => { console.log('SpeechRecognition: onspeechend'); addLog("éŸ³å£°ã®æ¤œå‡ºã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚", "info"); };

    // â˜…â˜…â˜… 3. onresult ã‚’ä¿®æ­£ â˜…â˜…â˜…
    recognition.onresult = (event) => {
      let final_transcript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        if (event.results[i].isFinal) {
          final_transcript += event.results[i][0].transcript;
        }
      }
      const text = final_transcript.trim();
      console.log('SpeechRecognition: onresult (final text)', text);
      if (text && websocket.current?.readyState === WebSocket.OPEN) {
        addLog(text, 'user');
        websocket.current.send(text);
        // â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼é€ä¿¡æ™‚ã«ã‚‚ã€Œè€ƒãˆä¸­ã€ã‚’é–‹å§‹
        setIsThinking(true); 
      } else if (text) {
        addLog(`(é€ä¿¡å¤±æ•—: ${text})`, 'info');
      }
    };

    recognition.onerror = (event) => {
      // ( ... å¤‰æ›´ãªã— ...)
      console.error('SpeechRecognition error:', event.error);
      let errorMsg = `éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${event.error}`;
      if (event.error === 'no-speech') errorMsg = 'éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚';
      else if (event.error === 'not-allowed') { errorMsg = 'ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'; setMicError(errorMsg); }
      else if (event.error === 'network' || event.error === 'service-not-allowed') errorMsg = 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¾ãŸã¯èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚';
      addLog(errorMsg, 'info');
      setIsThinking(false); // â˜… ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚è§£é™¤
    };
    return () => {
      if (recognition) { recognition.stop(); console.log("ChatPage unmount: SpeechRecognition stopped"); }
    };
  }, []); // ç©ºã®é…åˆ—ã§ãƒã‚¦ãƒ³ãƒˆæ™‚ã®ã¿

  const startMicrophone = () => {
    // ( ... å¤‰æ›´ãªã— ... )
    if (isRecording || !recognition) return;
    try { recognition.start(); } 
    catch (err) { console.error("recognition.start() ã‚¨ãƒ©ãƒ¼:", err); addLog('éŸ³å£°èªè­˜ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚', 'info'); }
  };

  const stopMicrophone = () => {
    // ( ... å¤‰æ›´ãªã— ... )
    if (!isRecording || !recognition) return;
    try { recognition.stop(); } 
    catch (err) { console.error("recognition.stop() ã‚¨ãƒ©ãƒ¼:", err); }
  };

  const toggleMicrophone = () => {
    // ( ... å¤‰æ›´ãªã— ... )
    if (isRecording) { stopMicrophone(); } 
    else { startMicrophone(); }
  };

  // â˜…â˜…â˜… 4. sendText ã‚’ä¿®æ­£ â˜…â˜…â˜…
  const sendText = () => {
    if (textInput && websocket.current?.readyState === WebSocket.OPEN) {
      websocket.current.send(textInput);
      addLog(textInput, 'user');
      setTextInput('');
      // â˜… ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡æ™‚ã«ã‚‚ã€Œè€ƒãˆä¸­ã€ã‚’é–‹å§‹
      setIsThinking(true); 
    }
  };


  // --- JSX (ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°) ---
  return (
    <div id="chat-page" className="page">
      <div className="chat-interface-area">
        {/* ( ... ãƒ˜ãƒƒãƒ€ãƒ¼ ... å¤‰æ›´ãªã—) */}
        <div className="header">
          <h1>ä¼šè©±</h1>
          <span id="status" className={status.key}>{status.text}</span>
          {status.key === 'connected' ? (
            <button id="disconnectButton" onClick={disconnect}>åˆ‡æ–­</button>
          ) : (
            <button id="connectButton" onClick={connect} disabled={status.key === 'connecting'}>
              {status.key === 'connecting' ? 'æ¥ç¶šä¸­...' : 'æ¥ç¶š'}
            </button>
          )}
        </div>
        
        {/* ( ... ãƒ­ã‚°ã‚³ãƒ³ãƒ†ãƒŠ ... ) */}
        <div className="log-container" ref={logContainerRef}>
          <div className="log-wrapper">
            {logs.map((log) => (
              <p key={log.id} className={`message ${log.type}`}>
                {log.text}
              </p>
            ))}
            
            {/* â˜…â˜…â˜… 5. isThinking ãŒ true ãªã‚‰ã€Œè€ƒãˆä¸­...ã€ã‚’è¡¨ç¤º â˜…â˜…â˜… */}
            {isThinking && (
              <p key="thinking-indicator" className="message info thinking-indicator">
                ï¼ˆè€ƒãˆä¸­...ï¼‰
              </p>
            )}
            
          </div>
        </div>
        
        {/* ( ... ãƒ•ãƒƒã‚¿ãƒ¼ ... å¤‰æ›´ãªã—) */}
        <div className="footer">
          <div className="input-area">
            <button
              id="micButton"
              className={isRecording ? 'recording' : ''}
              onClick={toggleMicrophone}
              disabled={status.key !== 'connected' || !!micError} 
            >
              {isRecording ? 'â– ' : 'ğŸ¤'} 
            </button>
            <input
              type="text"
              id="textInput"
              placeholder={micError ? micError : "ãƒ†ã‚­ã‚¹ãƒˆã§ã‚‚å…¥åŠ›ã§ãã¾ã™..."}
              value={textInput}
              onChange={(e) => setTextInput(e.target.value)}
              onKeyDown={(e) => e.key === 'Enter' && sendText()}
              disabled={status.key !== 'connected'}
            />
            <button id="sendButton" onClick={sendText} disabled={status.key !== 'connected'}>
              é€ä¿¡
            </button>
          </div>
        </div>
      </div>
      
      {/* ( ... VRMãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ ... å¤‰æ›´ãªã—) */}
      <div className="vrm-area">
        <VrmViewer ref={vrmViewerRef} audioData={audioToPlay} />
      </div>
    </div>
  );
}

export default ChatPage;